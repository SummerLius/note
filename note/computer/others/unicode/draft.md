<!-- TOC -->

- [草稿](#草稿)

<!-- /TOC -->

# 草稿

1. 程序操作的都是编码后的二进制，即存在的任何数据，都必须是编码后的二进制文本，例如utf8编码，而不是以码点二进制来保存。
    - 因为对文本（文本操作是基于字符单位，而不是二进制单位）进行操作，必须要从文本的二进制流中能识别除每个字符出来，如何确定二进制流中每个字符的二进制流的头和尾呢，这就是编码的作用，以一定的算法、规则，编码成新的二进制流，该二进制流可以用同样的规则确定每个字符
    - 问题来了，为什么不直接用编码后的数字来表示字符的码点code point呢？这样不就不需要所谓的utf8/16/32编码过程了，不更加直接，方便？
        - 这里就要考虑Unicode整体的设计，可以查看Unicode标准，查看其设计原则。
        - 考虑到Unicode的复杂任务，统一全球字符。其设计必须考虑到足够的因素，码点的区域的划分，类别的划分，大量字符的分配等等因素，字符对应的码点数字是单独设计的，为了码点数字良好的设计、组织，就没有考虑这个问题：`“多个码点表示的二进制数字连接到一起时，如何有效的识别每个码点的界限，不会造成混淆？”`，而是将这个问题单独用`编码形式`这个机制来处理。
        - 也就是说，Unicode的字符库和编码这两个功能设计是相互独立的，没有耦合
    - ASCII有时又说它是一种字符库，也说它是编码系统，为什么？
        - 解答：ascii含有的字符过于简单和少量，只需要单个字节 （只占7bit，首位为0）就可以代表其所有字符，所以其多个码点二进制组成一个序列时，可以很好的从其中识别除每个码点出来（判断首位为0），所以其不需要编码过程，其每个字符对应的码点，就是“编码”。